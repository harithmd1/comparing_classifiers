{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--path PATH] [--test-size TEST_SIZE]\n",
      "                             [--random-state RANDOM_STATE] [--outdir OUTDIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/pigeoneyevideography/Library/Jupyter/runtime/kernel-v378fb4745a58c9e7730e5df876f3f0ae2690eb613.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Bank Marketing: Model Benchmark (LR, Tree, KNN, SVM-RBF)\n",
    "- Auto-detects attached bank dataset (semicolon or comma separated)\n",
    "- Drops 'duration' to avoid target leakage\n",
    "- Preprocess: Standardize numeric, One-Hot encode categorical\n",
    "- Handles class imbalance via class_weight='balanced' where applicable\n",
    "- Tunes hyperparameters with 5-fold Stratified CV (scoring=ROC-AUC)\n",
    "- Reports Accuracy, F1, ROC-AUC, PR-AUC; saves summary + plots\n",
    "\n",
    "Usage:\n",
    "    python bank_benchmark.py\n",
    "Optional:\n",
    "    python bank_benchmark.py --path /mnt/data/bank-additional-full.csv\n",
    "    python bank_benchmark.py --test-size 0.2 --random-state 42\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# -------------------------\n",
    "# Data utilities\n",
    "# -------------------------\n",
    "CANDIDATES = [\n",
    "    \"bank-additional-full.csv\",\n",
    "    \"bank-full.csv\",\n",
    "    \"bank-additional.csv\",\n",
    "    \"bank.csv\",\n",
    "]\n",
    "\n",
    "def detect_delimiter(path: Path) -> str:\n",
    "    \"\"\"Try detecting delimiter (';' or ',') from first line.\"\"\"\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        head = f.readline()\n",
    "    if head.count(\";\") > head.count(\",\"):\n",
    "        return \";\"\n",
    "    return \",\"\n",
    "\n",
    "def find_dataset_path(cli_path: str | None) -> Path:\n",
    "    if cli_path:\n",
    "        p = Path(cli_path)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise FileNotFoundError(f\"Provided path not found: {cli_path}\")\n",
    "    data_root = Path(\"/mnt/data\")\n",
    "    for name in CANDIDATES:\n",
    "        p = data_root / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # fallback: any csv in /mnt/data\n",
    "    for p in data_root.glob(\"*.csv\"):\n",
    "        return p\n",
    "    raise FileNotFoundError(\"No CSV dataset found. Provide --path explicitly.\")\n",
    "\n",
    "def load_bank_dataset(path: Path) -> pd.DataFrame:\n",
    "    sep = detect_delimiter(path)\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    if \"y\" not in df.columns:\n",
    "        raise ValueError(\"Expected target column 'y' not found in dataset.\")\n",
    "    # Map y: {yes,no}->{1,0}\n",
    "    df[\"y\"] = df[\"y\"].astype(str).str.lower().eq(\"yes\").astype(int)\n",
    "    # Remove 'duration' (leakage per authors)\n",
    "    if \"duration\" in df.columns:\n",
    "        df = df.drop(columns=[\"duration\"])\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Modeling\n",
    "# -------------------------\n",
    "def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    num_cols = X.select_dtypes(include=[\"number\", \"float\", \"int\", \"bool\"]).columns.tolist()\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(with_mean=False), num_cols),   # sparse-safe\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ]\n",
    "    )\n",
    "    return pre, num_cols, cat_cols\n",
    "\n",
    "def bench_model(name, estimator, grid, pre, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Fit + CV tune; return metrics and artifacts.\"\"\"\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pipe = Pipeline([(\"prep\", pre), (\"clf\", estimator)])\n",
    "    gs = GridSearchCV(\n",
    "        pipe, param_grid=grid, cv=cv, scoring=\"roc_auc\",\n",
    "        n_jobs=-1, refit=True\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    gs.fit(X_train, y_train)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = gs.best_estimator_.predict(X_test)\n",
    "    y_proba = None\n",
    "    if hasattr(gs.best_estimator_[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"cv_best_score(ROC-AUC)\": float(gs.best_score_),\n",
    "        \"test_acc\": float(accuracy_score(y_test, y_pred)),\n",
    "        \"test_f1\": float(f1_score(y_test, y_pred)),\n",
    "        \"test_roc_auc\": float(roc_auc_score(y_test, y_proba)) if y_proba is not None else np.nan,\n",
    "        \"test_pr_auc\": float(average_precision_score(y_test, y_proba)) if y_proba is not None else np.nan,\n",
    "        \"fit_time_s\": float(t1 - t0),\n",
    "        \"best_params\": gs.best_params_,\n",
    "        \"report\": classification_report(y_test, y_pred, digits=3),\n",
    "        \"cm\": confusion_matrix(y_test, y_pred).tolist(),\n",
    "        \"has_proba\": y_proba is not None,\n",
    "        \"best_estimator\": gs.best_estimator_,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_time_vs_auc(summary_df: pd.DataFrame, title: str, outpath: Path):\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    plt.scatter(summary_df[\"fit_time_s\"], summary_df[\"test_roc_auc\"])\n",
    "    for _, row in summary_df.iterrows():\n",
    "        plt.annotate(row[\"model\"], (row[\"fit_time_s\"], row[\"test_roc_auc\"]),\n",
    "                     xytext=(5, 5), textcoords=\"offset points\")\n",
    "    plt.xlabel(\"Fit time (s)\")\n",
    "    plt.ylabel(\"Test ROC-AUC\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def plot_curves(best_estimator, X_test, y_test, name: str, outdir: Path):\n",
    "    if not hasattr(best_estimator[\"clf\"], \"predict_proba\"):\n",
    "        return\n",
    "    proba = best_estimator.predict_proba(X_test)[:, 1]\n",
    "    # PR curve\n",
    "    prec, rec, _ = precision_recall_curve(y_test, proba)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR Curve ({name})\")\n",
    "    plt.tight_layout(); plt.savefig(outdir / \"pr_curve.png\", dpi=160); plt.close()\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc(fpr, tpr):.3f}\")\n",
    "    plt.plot([0,1], [0,1], \"--\", alpha=0.5)\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC Curve ({name})\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(outdir / \"roc_curve.png\", dpi=160); plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Bank Marketing Benchmark (LR, Tree, KNN, SVM-RBF)\")\n",
    "    p.add_argument(\"--path\", type=str, default=None, help=\"Path to bank dataset CSV\")\n",
    "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set size fraction\")\n",
    "    p.add_argument(\"--random-state\", type=int, default=42, help=\"Random state for reproducibility\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=\"bank_benchmark_outputs\", help=\"Output directory\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    outdir = Path(args.outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data_path = find_dataset_path(args.path)\n",
    "    print(f\"[INFO] Using dataset: {data_path}\")\n",
    "\n",
    "    df = load_bank_dataset(data_path)\n",
    "    y = df[\"y\"].values\n",
    "    X = df.drop(columns=[\"y\"])\n",
    "\n",
    "    print(f\"[INFO] Dataset shape: {df.shape} | Positive rate: {y.mean():.3f}\")\n",
    "\n",
    "    pre, num_cols, cat_cols = build_preprocessor(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=args.test_size, stratify=y, random_state=args.random_state\n",
    "    )\n",
    "\n",
    "    # Define models and grids\n",
    "    models = [\n",
    "        (\"LogisticRegression\",\n",
    "         LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "         {\"clf__C\": [0.1, 1, 10, 100]}),\n",
    "\n",
    "        (\"DecisionTree\",\n",
    "         DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"),\n",
    "         {\"clf__max_depth\": [None, 6, 10, 14],\n",
    "          \"clf__min_samples_leaf\": [1, 5, 10]}),\n",
    "\n",
    "        (\"KNN\",\n",
    "         KNeighborsClassifier(),\n",
    "         {\"clf__n_neighbors\": [5, 9, 15, 25],\n",
    "          \"clf__weights\": [\"uniform\", \"distance\"]}),\n",
    "\n",
    "        (\"SVC(RBF)\",\n",
    "         SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=True),\n",
    "         {\"clf__C\": [0.5, 1, 5, 10],\n",
    "          \"clf__gamma\": [0.001, 0.01, 0.1]}),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, est, grid in models:\n",
    "        print(f\"[INFO] Tuning {name} ...\")\n",
    "        res = bench_model(name, est, grid, pre, X_train, y_train, X_test, y_test)\n",
    "        results.append(res)\n",
    "        print(f\"[DONE] {name}: \"\n",
    "              f\"ROC-AUC={res['test_roc_auc']:.3f}, PR-AUC={res['test_pr_auc']:.3f}, \"\n",
    "              f\"ACC={res['test_acc']:.3f}, F1={res['test_f1']:.3f}, \"\n",
    "              f\"time={res['fit_time_s']:.2f}s\")\n",
    "\n",
    "    # Summary table\n",
    "    summary = pd.DataFrame([\n",
    "        {k: v for k, v in r.items()\n",
    "         if k in [\"model\", \"cv_best_score(ROC-AUC)\", \"test_roc_auc\", \"test_pr_auc\",\n",
    "                  \"test_acc\", \"test_f1\", \"fit_time_s\", \"best_params\"]}\n",
    "        for r in results\n",
    "    ]).sort_values([\"test_roc_auc\", \"test_pr_auc\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    summary_path = outdir / \"summary.csv\"\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    print(f\"\\n[INFO] Saved summary -> {summary_path}\\n\")\n",
    "    print(summary)\n",
    "\n",
    "    # Best model details\n",
    "    best_name = summary.iloc[0][\"model\"]\n",
    "    best = next(r for r in results if r[\"model\"] == best_name)\n",
    "    print(\"\\n[BEST MODEL]\", best_name)\n",
    "    print(\"Best params:\", best[\"best_params\"])\n",
    "    print(\"Confusion matrix:\\n\", np.array(best[\"cm\"]))\n",
    "    print(best[\"report\"])\n",
    "\n",
    "    # Plots\n",
    "    plot_time_vs_auc(summary, f\"Trade-off on {data_path.name}\", outdir / \"time_vs_roc_auc.png\")\n",
    "    plot_curves(best[\"best_estimator\"], X_test, y_test, best_name, outdir)\n",
    "\n",
    "    print(f\"\\n[INFO] Saved plots to: {outdir}/\")\n",
    "    print(\" - time_vs_roc_auc.png\")\n",
    "    if best.get(\"has_proba\", False):\n",
    "        print(\" - pr_curve.png\")\n",
    "        print(\" - roc_curve.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
